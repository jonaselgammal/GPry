{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introductory example to using GPry\n",
    "----------------------------------\n",
    "\n",
    "This notebook will guide you through the basic steps for sampling a likelihood with GPry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Setting up a likelihood function\n",
    "\n",
    "Let's set up a simple 2d Gaussian likelihood as an example:\n",
    "\n",
    "$$y(x) \\sim \\mathcal{N}(x|\\mu,\\Sigma)$$\n",
    "\n",
    "with \n",
    "$$\\mu=\\begin{pmatrix}3 \\\\ 2\\end{pmatrix},\\ \\Sigma=\\begin{pmatrix}0.5 & 0.4 \\\\ 0.4 & 1.5\\end{pmatrix}$$\n",
    "\n",
    "**Note that we need to pass GPry the log-Likelihood!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "mean = [3, 2]\n",
    "cov = [[0.5, 0.4], [0.4, 1.5]]\n",
    "rv = multivariate_normal(mean, cov)\n",
    "\n",
    "def logLkl(x_1, x_2):\n",
    "    return rv.logpdf(np.array([x_1, x_2]).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and some bounds which we set to $[-10, 10]$ for both paramters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds = [[-10, 10], [-10, 10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Creating the Runner object\n",
    "\n",
    "The `run.Runner` manages model specification and the active sampling loop of GPry up to convergence, as well as allows for some post-processing and tests.\n",
    "\n",
    "To initialise it, we pass it the log-likelihood function as first argument, and the prior bounds via the ``bounds`` keyword (more complicated prior specifications can be used by defining and passing as first argument a `Cobaya` model).\n",
    "\n",
    "Optionally, we can also also pass a path to save checkpoints via the ``checkpoint`` argument. \n",
    "\n",
    "If passed, in order to prevent loss of data, you **must** decide a checkpoint policy (either ``\"resume\"`` or ``\"overwrite\"``). \n",
    "\n",
    "- If set to ``\"resume\"`` the runner object will try to load the checkpoint and resume the active sampling loop from there\n",
    "- If set to ``\"overwrite\"`` it will start from scratch and overwrite checkpoint files which already exist.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpry.run import Runner\n",
    "checkpoint = \"output/simple\"\n",
    "runner = Runner(logLkl, bounds, checkpoint=checkpoint, load_checkpoint=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Running the active learning loop\n",
    "\n",
    "Since all training parameters are chosen automatically, all we have to do is to call the\n",
    "`run` function of the runner object:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will run the active sampling loop until convergence is reached. \n",
    "\n",
    "It also saves the checkpoint files after every iteration of the bayesian optimization loop and creates progress plots which are saved in `[checkpoint]/images/` (`./images/` if checkpoint is None).\n",
    "\n",
    "Once converged, you can access the surrogate model and use it as a function for any purpose. \n",
    "\n",
    "Note that internally GPry models the **log-posterior**, not the log-likelihood and that you need to hand GPry a single (nsamples, ndim) with the locations where you want to evaluate the surrogate.\n",
    "\n",
    "Nevertheless we can call either `runner.logp` or `runner.logL`\n",
    "\n",
    "Let us compare GPry and the likelihood in the location (1, 2): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point = (1, 2)\n",
    "print(f\"Log-lkl at (1,2): {logLkl(*point)}\")\n",
    "print(f\"surrogate at (1,2): {runner.logL(point)[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both evaluations should produce similar numbers.\n",
    "\n",
    "Congratulations, now you have a surrogate of your posterior distribution!!!\n",
    "\n",
    "----\n",
    "\n",
    "Now let's see how you make corner plots and get marginalised quantities.\n",
    "\n",
    "### Step 4: Monte Carlo samples on the surrogate model\n",
    "\n",
    "The ``Runner`` object can also run an MC sampler on the GP in order to extract marginalised quantities. To do that, we use the ``generate_mc_sample`` method of the ``Runner``.\n",
    "\n",
    "By default, GPry would already have run an MC sampler at the end of the main loop, for diagnosis purposes. You can get the result using the ``last_mc_samples`` method, which returns a dictionary containing the samples' parameter values, weight (``None`` if all samples carry equal weight), and values for the surrogate log-posterior, true log- prior, and surrogate log-likelihood (i.e., the surrogate log-posterior minus the analytic log-prior):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_samples_dict = runner.last_mc_samples(as_pandas=True)\n",
    "print(mc_samples_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Samples are also stored by default in the same folder as the checkpoint, inside a ``chains`` sub folder. The order of the columns in that file are ``weight log-posterior param_1 param_2 ...``.\n",
    "\n",
    "Subsequent calls to the ``generate_mc_sample`` method can be used to re-generate MC samples from the surrogate posterior, e.g. if a finer representation is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner.generate_mc_sample(\n",
    "    # Example args for denser samples\n",
    "    # sampler={\"nested\": {\"nlive\": \"25d\", \"num_repeats\": \"10d\"}}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the results\n",
    "\n",
    "Now that we have MC samples you can process and plot them the same way that you would do with any other MC samples.\n",
    "\n",
    "The easiest way to get a corner plot though is to call the `plot_mc` method of the ``Runner`` object which will generate a corner plot.\n",
    "\n",
    "It includes the training set unless passed ``add_training=False``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner.plot_mc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus: Getting some extra insights\n",
    "\n",
    "You can do further plots about the progress of the active-learning loop using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner.plot_progress()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This results in the following: \n",
    "- a histogram of the distribution time spent at different parts of the code (`timing.png`)\n",
    "- the distribution of the training samples (`trace.png`)\n",
    "- A plot showing the value(s) of all convergence criteria as function of the number of posterior evaluations (`convergence.png`)\n",
    "\n",
    "The plots are saved in `[checkpoint]/images/` (`./images/` if checkpoint is None).\n",
    "\n",
    "### Bonus Bonus: Validation\n",
    "\n",
    "**This part is optional and only relevant for validating the contours that GPry produces. In a realistic scenario you would obviously not run a full MCMC on the likelihood**\n",
    "\n",
    "To compare our contours to the true Gaussian we draw 10000 samples from it, and set them as *fiducial samples* in the ``Runner``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth_samples = rv.rvs(size=10000)\n",
    "runner.set_fiducial_MC(truth_samples)\n",
    "\n",
    "runner.plot_mc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gprybalrogrelease",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
